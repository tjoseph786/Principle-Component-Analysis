{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "lab_3.py",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ERv1pFB14C6M"
      },
      "source": [
        "#Q1."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UnOBw-pX4IVK"
      },
      "source": [
        "###Q1-->Part 1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Utlomo483tgq",
        "outputId": "d09d06e5-35b6-4848-d43d-60bf1287fa86",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from sklearn import linear_model\n",
        "import pandas as pd\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "\n",
        "df = pd.read_excel('slr06.xls')\n",
        "# print(df)\n",
        "\n",
        "#  set X  and  y seperately\n",
        "X_data = df[['X']]\n",
        "y_data = df['Y']\n",
        "\n",
        "# calculating the number of rows for the diving in training, validation,test\n",
        "per_60 = int(len(X_data)*(60/100))\n",
        "per_val_20= int(len(X_data)*(80/100))\n",
        "per_test_20 = int(len(X_data))\n",
        "\n",
        "\n",
        "\n",
        "#  training sets \n",
        "X_train = X_data[:per_60]\n",
        "y_train = y_data[:per_60]\n",
        "print('training settd are')\n",
        "print(X_train,y_train)\n",
        "\n",
        "# validation set\n",
        "X_var = X_data[per_60+1:per_val_20]\n",
        "y_var = y_data[per_60+1:per_val_20]\n",
        "print('validating sets are')\n",
        "print(X_var,y_var)\n",
        "\n",
        "# test set\n",
        "X_test = X_data[per_val_20+1:per_test_20]\n",
        "y_test = y_data[per_val_20+1:per_test_20]\n",
        "print('test sets are ')\n",
        "print(X_test,y_test)\n",
        "\n",
        "#  creting model object\n",
        "model = linear_model.LinearRegression()\n",
        "\n",
        "# fitting or traing the model with the training sets\n",
        "model.fit(X_train,y_train)\n",
        "\n",
        "\n",
        "\n",
        "#  predictions of the data from the model\n",
        "\n",
        "#  prediction of train sets\n",
        "predict_y_train = model.predict(X_train)\n",
        "\n",
        "\n",
        "# prediction of validation set\n",
        "predict_y_validation = model.predict(X_var)\n",
        "\n",
        "# prediction of the test sets\n",
        "predict_y_test = model.predict(X_test)\n",
        "\n",
        "\n",
        "#  calculating MAE --> mean absolute error\n",
        "\n",
        "# MAE of the train set\n",
        "mae_train = mean_absolute_error(predict_y_train, y_train)\n",
        "\n",
        "# MAE of the validation set\n",
        "mae_val = mean_absolute_error(predict_y_validation, y_var)\n",
        "\n",
        "# MAE of the test set\n",
        "mae_test = mean_absolute_error(predict_y_test, y_var)\n",
        "\n",
        "print('Mean absolute error of train sets',mae_train)\n",
        "print('Mean absolute error of validation sets',mae_val)\n",
        "print('Mean absolute error of test sets',mae_test)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "*** No CODEPAGE record, no encoding_override: will use 'ascii'\n",
            "training settd are\n",
            "      X\n",
            "0   108\n",
            "1    19\n",
            "2    13\n",
            "3   124\n",
            "4    40\n",
            "5    57\n",
            "6    23\n",
            "7    14\n",
            "8    45\n",
            "9    10\n",
            "10    5\n",
            "11   48\n",
            "12   11\n",
            "13   23\n",
            "14    7\n",
            "15    2\n",
            "16   24\n",
            "17    6\n",
            "18    3\n",
            "19   23\n",
            "20    6\n",
            "21    9\n",
            "22    9\n",
            "23    3\n",
            "24   29\n",
            "25    7\n",
            "26    4\n",
            "27   20\n",
            "28    7\n",
            "29    4\n",
            "30    0\n",
            "31   25\n",
            "32    6\n",
            "33    5\n",
            "34   22\n",
            "35   11\n",
            "36   61 0     392.5\n",
            "1      46.2\n",
            "2      15.7\n",
            "3     422.2\n",
            "4     119.4\n",
            "5     170.9\n",
            "6      56.9\n",
            "7      77.5\n",
            "8     214.0\n",
            "9      65.3\n",
            "10     20.9\n",
            "11    248.1\n",
            "12     23.5\n",
            "13     39.6\n",
            "14     48.8\n",
            "15      6.6\n",
            "16    134.9\n",
            "17     50.9\n",
            "18      4.4\n",
            "19    113.0\n",
            "20     14.8\n",
            "21     48.7\n",
            "22     52.1\n",
            "23     13.2\n",
            "24    103.9\n",
            "25     77.5\n",
            "26     11.8\n",
            "27     98.1\n",
            "28     27.9\n",
            "29     38.1\n",
            "30      0.0\n",
            "31     69.2\n",
            "32     14.6\n",
            "33     40.3\n",
            "34    161.5\n",
            "35     57.2\n",
            "36    217.6\n",
            "Name: Y, dtype: float64\n",
            "validating sets are\n",
            "     X\n",
            "38   4\n",
            "39  16\n",
            "40  13\n",
            "41  60\n",
            "42  41\n",
            "43  37\n",
            "44  55\n",
            "45  41\n",
            "46  11\n",
            "47  27\n",
            "48   8\n",
            "49   3 38     12.6\n",
            "39     59.6\n",
            "40     89.9\n",
            "41    202.4\n",
            "42    181.3\n",
            "43    152.8\n",
            "44    162.8\n",
            "45     73.4\n",
            "46     21.3\n",
            "47     92.6\n",
            "48     76.1\n",
            "49     39.9\n",
            "Name: Y, dtype: float64\n",
            "test sets are \n",
            "     X\n",
            "51  13\n",
            "52  13\n",
            "53  15\n",
            "54   8\n",
            "55  29\n",
            "56  30\n",
            "57  24\n",
            "58   9\n",
            "59  31\n",
            "60  14\n",
            "61  53\n",
            "62  26 51     93.0\n",
            "52     31.9\n",
            "53     32.1\n",
            "54     55.6\n",
            "55    133.3\n",
            "56    194.5\n",
            "57    137.9\n",
            "58     87.4\n",
            "59    209.8\n",
            "60     95.5\n",
            "61    244.6\n",
            "62    187.5\n",
            "Name: Y, dtype: float64\n",
            "Mean absolute error of train sets 23.31027790380166\n",
            "Mean absolute error of validation sets 27.202386638114906\n",
            "Mean absolute error of test sets 62.67760941818835\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AInAeY6T5C7a"
      },
      "source": [
        "###Q2--->Part 2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KF-Si2qn4-RW",
        "outputId": "9eeabeee-2878-42ff-a6bd-e5ab0169f49d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "import sys\n",
        "import pandas as pd\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "\n",
        "limit = 10**8\n",
        "sys.setrecursionlimit(limit)\n",
        "\n",
        "def StepGradient(m_current, c_current, lr, df):\n",
        "    #  we want to find where the error is smallest\n",
        "    c_gradient = 0\n",
        "    m_gradient = 0\n",
        "    N = int(len(df))\n",
        "    \n",
        "    #  calculation fr the -2/n(derivative w.r.t the slope and w.r.t the intercept)\n",
        "    for x in range(0, len(df)):\n",
        "        X_next = df.iloc[x, 0]\n",
        "        y_next = df.iloc[x, 1]\n",
        "\n",
        "        # predicted y value\n",
        "        y_predicted = m_current * X_next\n",
        "\n",
        "        m_gradient+= (-2)*(X_next * (y_next - y_predicted))/N\n",
        "        c_gradient+= (-2)*(y_next - y_predicted)/N\n",
        "    \n",
        "    # new c value and m value\n",
        "    new_c = c_current - (lr * c_gradient)\n",
        "    new_m = m_current - (lr * m_gradient)\n",
        "\n",
        "    return (round(new_c,2), round(new_m,2))\n",
        "\n",
        "    \n",
        "def GDA(df, m, c, lr, times):\n",
        "    for x in range(times):\n",
        "        c, m = StepGradient(m, c, lr, df)\n",
        "\n",
        "    return (m, c)\n",
        "\n",
        "def run(df):\n",
        "    \n",
        "\n",
        "#  hyper_parameter that is learning rate for the gradient descent algorithm\n",
        "    learning_rate = 0.0001\n",
        "\n",
        "    # m is the slpoe and c is the intercept\n",
        "    #  y = mx + c\n",
        "    m_start = 0\n",
        "    c_start = 0\n",
        "\n",
        "    #  iterating number of times\n",
        "    times = 1000\n",
        "\n",
        "    per_60 = int(len(df)*(60/100))\n",
        "    data = df[:per_60]\n",
        "\n",
        "    slope, intercept = GDA(data, m_start, c_start, learning_rate, times)\n",
        "\n",
        "    # print(slope, intercept)\n",
        "    return (slope, intercept)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    df = pd.read_excel('slr06.xls')\n",
        "    slope,intercept = run(df)\n",
        "\n",
        "    df = pd.read_excel('slr06.xls')\n",
        "    #  set X  and  y seperately\n",
        "    X_data = df[['X']]\n",
        "    y_data = df['Y']\n",
        "\n",
        "    # print(X_data)\n",
        "\n",
        "    # calculating the number of rows for the diving in training, validation,test\n",
        "    per_60 = int(len(X_data)*(60/100))\n",
        "    per_val_20= int(len(X_data)*(80/100))\n",
        "    per_test_20 = int(len(X_data))\n",
        "\n",
        "\n",
        "\n",
        "    #  training sets \n",
        "    X_train = X_data[:per_60]\n",
        "    y_train = y_data[:per_60]\n",
        "    print('training settd are')\n",
        "    print(X_train,y_train)\n",
        "    \n",
        "    # validation set\n",
        "    X_var = X_data[per_60+1:per_val_20]\n",
        "    y_var = y_data[per_60+1:per_val_20]\n",
        "    print('validating sets are')\n",
        "    print(X_var,y_var)\n",
        "\n",
        "    # test set\n",
        "    X_test = X_data[per_val_20+1:per_test_20]\n",
        "    y_test = y_data[per_val_20+1:per_test_20]\n",
        "    print('test sets are ')\n",
        "    print(X_test,y_test)\n",
        "\n",
        "\n",
        "\n",
        "    # cal the predicted y train , validate and the test\n",
        "\n",
        "    train = [(slope*float(X_train.iloc[i,0]) + intercept) for i in range(len(X_train))]\n",
        "    val = [(slope*float(X_var.iloc[i,0]) + intercept) for i in range(len(X_var))]\n",
        "    test = [(slope*float(X_test.iloc[i,0]) + intercept) for i in range(len(X_test))]\n",
        "    \n",
        "\n",
        "    print('mean absolute error for the trainging set',mean_absolute_error(train, y_train))\n",
        "    print('mean absolute error for the validation set',mean_absolute_error(val, y_var))\n",
        "    print('mean absolute error for the testing set',mean_absolute_error(test, y_test))\n",
        "\n",
        "\n",
        "\n",
        "#  plotting the graph to check\n",
        "    import matplotlib.pyplot as plt\n",
        "    plt.scatter(X_data, y_data)\n",
        "    plt.plot(X_train,train,color = 'red')\n",
        "    plt.show()\n",
        "\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "*** No CODEPAGE record, no encoding_override: will use 'ascii'\n",
            "*** No CODEPAGE record, no encoding_override: will use 'ascii'\n",
            "training settd are\n",
            "      X\n",
            "0   108\n",
            "1    19\n",
            "2    13\n",
            "3   124\n",
            "4    40\n",
            "5    57\n",
            "6    23\n",
            "7    14\n",
            "8    45\n",
            "9    10\n",
            "10    5\n",
            "11   48\n",
            "12   11\n",
            "13   23\n",
            "14    7\n",
            "15    2\n",
            "16   24\n",
            "17    6\n",
            "18    3\n",
            "19   23\n",
            "20    6\n",
            "21    9\n",
            "22    9\n",
            "23    3\n",
            "24   29\n",
            "25    7\n",
            "26    4\n",
            "27   20\n",
            "28    7\n",
            "29    4\n",
            "30    0\n",
            "31   25\n",
            "32    6\n",
            "33    5\n",
            "34   22\n",
            "35   11\n",
            "36   61 0     392.5\n",
            "1      46.2\n",
            "2      15.7\n",
            "3     422.2\n",
            "4     119.4\n",
            "5     170.9\n",
            "6      56.9\n",
            "7      77.5\n",
            "8     214.0\n",
            "9      65.3\n",
            "10     20.9\n",
            "11    248.1\n",
            "12     23.5\n",
            "13     39.6\n",
            "14     48.8\n",
            "15      6.6\n",
            "16    134.9\n",
            "17     50.9\n",
            "18      4.4\n",
            "19    113.0\n",
            "20     14.8\n",
            "21     48.7\n",
            "22     52.1\n",
            "23     13.2\n",
            "24    103.9\n",
            "25     77.5\n",
            "26     11.8\n",
            "27     98.1\n",
            "28     27.9\n",
            "29     38.1\n",
            "30      0.0\n",
            "31     69.2\n",
            "32     14.6\n",
            "33     40.3\n",
            "34    161.5\n",
            "35     57.2\n",
            "36    217.6\n",
            "Name: Y, dtype: float64\n",
            "validating sets are\n",
            "     X\n",
            "38   4\n",
            "39  16\n",
            "40  13\n",
            "41  60\n",
            "42  41\n",
            "43  37\n",
            "44  55\n",
            "45  41\n",
            "46  11\n",
            "47  27\n",
            "48   8\n",
            "49   3 38     12.6\n",
            "39     59.6\n",
            "40     89.9\n",
            "41    202.4\n",
            "42    181.3\n",
            "43    152.8\n",
            "44    162.8\n",
            "45     73.4\n",
            "46     21.3\n",
            "47     92.6\n",
            "48     76.1\n",
            "49     39.9\n",
            "Name: Y, dtype: float64\n",
            "test sets are \n",
            "     X\n",
            "51  13\n",
            "52  13\n",
            "53  15\n",
            "54   8\n",
            "55  29\n",
            "56  30\n",
            "57  24\n",
            "58   9\n",
            "59  31\n",
            "60  14\n",
            "61  53\n",
            "62  26 51     93.0\n",
            "52     31.9\n",
            "53     32.1\n",
            "54     55.6\n",
            "55    133.3\n",
            "56    194.5\n",
            "57    137.9\n",
            "58     87.4\n",
            "59    209.8\n",
            "60     95.5\n",
            "61    244.6\n",
            "62    187.5\n",
            "Name: Y, dtype: float64\n",
            "mean absolute error for the trainging set 23.368378378378374\n",
            "mean absolute error for the validation set 27.275000000000006\n",
            "mean absolute error for the testing set 50.800000000000004\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nz4E4_81E2u8"
      },
      "source": [
        "#Q2."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l4JziER55Mp5",
        "outputId": "22bfa14f-30a2-4f39-f6cc-e401d806cd2f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 697
        }
      },
      "source": [
        "from sklearn.datasets import load_boston\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "\n",
        "boston_dataset = load_boston(return_X_y=False)\n",
        "#print(boston_dataset.keys())\n",
        "#print(boston_dataset.DESCR)\n",
        "boston = pd.DataFrame(boston_dataset.data, columns=boston_dataset.feature_names)\n",
        "boston['MEDV'] = boston_dataset.target # Median value of owner-occupied homes in $1000s ( we have to predict it )\n",
        "#boston.head(10)\n",
        "data = boston.to_numpy() #converting DataFrames to numpy array\n",
        "print(\"boston data shape (row,column) : \",data.shape)\n",
        "\n",
        "#dividing data into training set , validation set and test set\n",
        "sz = data[:,0].size\n",
        "train_data = data[0:int(sz*0.6),:]\n",
        "validate_data = data[int(sz*0.6) : int(sz*0.8)   , :]\n",
        "test_data = data[int(sz*0.8)  : , :]\n",
        "print(\"total_data : \" ,sz)\n",
        "print(\"train_data : \" , train_data[:,0].size)\n",
        "print(\"validation_data : \" ,validate_data[:,0].size)\n",
        "print(\"test_data : \" ,test_data[:,0].size) \n",
        "\n",
        "#======================================================================\n",
        "\n",
        "# gradientDescent function to calculate optimal theta\n",
        "def gradientDescent(X, y, theta, alpha, num_iters):\n",
        "    m = y.size  # number of training examples\n",
        "    for i in range(num_iters):\n",
        "        y_hat = np.dot(X, theta) # y_predicted (predicted housing price)\n",
        "        theta = theta - alpha * (1.0/m) * np.dot(X.T, y_hat-y) \n",
        "    return theta\n",
        "\n",
        "\n",
        "# adding column of ones to the data sets for theta0 ( i.e. the first parameter int theta )\n",
        "n=train_data[:,0].size\n",
        "import numpy as np\n",
        "ones=np.ones((n,1))\n",
        "train_data = np.column_stack((ones,train_data)) # or like -> np.c_[ones, train_data]\n",
        "\n",
        "n=validate_data[:,0].size\n",
        "ones=np.ones((n,1))\n",
        "validate_data = np.column_stack((ones,validate_data))\n",
        "\n",
        "n=test_data[:,0].size\n",
        "ones=np.ones((n,1))\n",
        "test_data = np.column_stack((ones,test_data))\n",
        "\n",
        "\n",
        "\n",
        "print(\"=========== GRADIENT DESCENT ALGORITHM STARTS : ==============\")\n",
        "nrow,ncol = data.shape\n",
        "x = train_data[:,0:ncol]\n",
        "y = train_data[:,ncol:]\n",
        "theta = np.zeros((ncol,1))\n",
        "alpha = 0.000001\n",
        "iter = 100000\n",
        "theta = gradientDescent(x,y,theta,alpha,iter)\n",
        "print(\"theta from training set : \",theta)\n",
        "y_predicted = np.dot(x,theta) \n",
        "mae = mean_absolute_error(y_predicted , y)\n",
        "\n",
        "# tuning the hyper-parameter (alpha) using validation data_set\n",
        "values_of_alpha = [0.000003 , 0.000001, 0.0000003 , 0.00000001 , 0.00000003]\n",
        "min_mae = mae\n",
        "res_alpha = alpha\n",
        "res_theta = theta\n",
        "x = validate_data[:,0:ncol]\n",
        "y = validate_data[:,ncol:]\n",
        "\n",
        "for alpha in values_of_alpha: # finding the best value of alpha which gives minimum value of MAE on validation set\n",
        "  theta = np.zeros((ncol,1))\n",
        "  theta = gradientDescent(x,y,theta,alpha,iter)\n",
        "  y_predicted = np.dot(x,theta) \n",
        "  mae = mean_absolute_error(y_predicted , y)\n",
        "  if mae < min_mae:\n",
        "    res_alpha = alpha\n",
        "    res_theta = theta\n",
        "    min_mae = mae\n",
        "\n",
        "theta = res_theta\n",
        "print(\"theta after tuning alpha from validation set : \",theta)\n",
        " \n",
        "\n",
        "# calculating accuracy on train data \n",
        "x = train_data[:,0:ncol]\n",
        "y = train_data[:,ncol:]\n",
        "\n",
        "y_predicted = np.dot(x,theta) \n",
        "mae = mean_absolute_error(y_predicted , y)\n",
        "print(\"MEAN ABSOLUTE ERROR ON TRAINING DATA : \",mae)\n",
        "\n",
        "\n",
        "# calculating accuracy on validation data \n",
        "x = validate_data[:,0:ncol]\n",
        "y = validate_data[:,ncol:]\n",
        "\n",
        "y_predicted = np.dot(x,theta) \n",
        "mae = mean_absolute_error(y_predicted , y)\n",
        "print(\"MEAN ABSOLUTE ERROR ON VALIDATING DATA : \",mae)\n",
        "\n",
        "\n",
        "# calculating accuracy on test data \n",
        "x = test_data[:,0:ncol]\n",
        "y = test_data[:,ncol:]\n",
        "\n",
        "y_predicted = np.dot(x,theta) \n",
        "mae = mean_absolute_error(y_predicted , y)\n",
        "print(\"MEAN ABSOLUTE ERROR ON TEST  DATA : \",mae)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "boston data shape (row,column) :  (506, 14)\n",
            "total_data :  506\n",
            "train_data :  303\n",
            "validation_data :  101\n",
            "test_data :  102\n",
            "=========== GRADIENT DESCENT ALGORITHM STARTS : ==============\n",
            "theta from training set :  [[ 0.02277276]\n",
            " [ 0.07891455]\n",
            " [ 0.06918869]\n",
            " [-0.01713093]\n",
            " [ 0.02599327]\n",
            " [ 0.0184991 ]\n",
            " [ 0.44055185]\n",
            " [ 0.08587467]\n",
            " [-0.15999092]\n",
            " [ 0.17182027]\n",
            " [-0.00092437]\n",
            " [-0.15714102]\n",
            " [ 0.07412098]\n",
            " [-0.85012851]]\n",
            "theta after tuning alpha from validation set :  [[ 0.04225091]\n",
            " [-0.12855066]\n",
            " [ 0.06273439]\n",
            " [-0.07674999]\n",
            " [ 0.10012007]\n",
            " [-0.00796301]\n",
            " [ 0.10373847]\n",
            " [ 0.08264848]\n",
            " [-0.23843666]\n",
            " [ 0.18461958]\n",
            " [ 0.004664  ]\n",
            " [ 0.483971  ]\n",
            " [ 0.0399667 ]\n",
            " [-1.02870376]]\n",
            "MEAN ABSOLUTE ERROR ON TRAINING DATA :  6.227073327937616\n",
            "MEAN ABSOLUTE ERROR ON VALIDATING DATA :  4.592752365421676\n",
            "MEAN ABSOLUTE ERROR ON TEST  DATA :  6.030333031725009\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f1AVfmxnExtn"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}